{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([\"a\",\"e\",\"f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.map(lambda a: (a,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('e', 1), ('f', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = sc.parallelize([1,4,7,8])\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8]\n"
     ]
    }
   ],
   "source": [
    "y1 = x1.filter(lambda a: a%2==0)\n",
    "print(y1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10, 4, 40, 7, 70, 8, 80]\n"
     ]
    }
   ],
   "source": [
    "y2 = x1.flatMap(lambda a: (a,a*10))\n",
    "print(y2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GroupBy\n",
    "\n",
    "x2 = sc.parallelize(['Anil','Laddu','Anukush','Beru'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = x2.groupBy(lambda a: a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', <pyspark.resultiterable.ResultIterable object at 0x7fdae0c19b38>), ('L', <pyspark.resultiterable.ResultIterable object at 0x7fdae0c19ba8>), ('A', <pyspark.resultiterable.ResultIterable object at 0x7fdae0c19cf8>)]\n"
     ]
    }
   ],
   "source": [
    "print(y2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', ['Beru']), ('L', ['Laddu']), ('A', ['Anil', 'Anukush'])]\n"
     ]
    }
   ],
   "source": [
    "print([(k, list(v)) for (k,v) in y2.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', ['Anil', 'Anukush']), ('B', ['Beru']), ('L', ['Laddu'])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, sorted(y)) for (x, y) in y2.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = sc.parallelize([('A',3),('B',3),('C',4),('A',6),('C',8),('D',8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = g1.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', <pyspark.resultiterable.ResultIterable at 0x7fdae0c157f0>),\n",
       " ('C', <pyspark.resultiterable.ResultIterable at 0x7fdae0c15f60>),\n",
       " ('A', <pyspark.resultiterable.ResultIterable at 0x7fdae0c15dd8>),\n",
       " ('D', <pyspark.resultiterable.ResultIterable at 0x7fdae0c15ef0>)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', [3]), ('C', [4, 8]), ('A', [3, 6]), ('D', [8])]\n"
     ]
    }
   ],
   "source": [
    "print(list((k[0], list(k[1]))  for k in h1.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', [3]), ('C', [4, 8]), ('A', [3, 6]), ('D', [8])]\n"
     ]
    }
   ],
   "source": [
    "print([(k[0], list(k[1]))  for k in h1.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 3), ('C', 12), ('A', 9), ('D', 8)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = h1.map(lambda a: (a[0],sum(a[1])))\n",
    "h2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduceByKey\n",
    "\n",
    "from operator import add\n",
    "\n",
    "rbk1 = g1.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 3), ('C', 12), ('A', 9), ('D', 8)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbk1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGeneratorFun(): \n",
    "    yield 1;     yield 2;     yield 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for value in simpleGeneratorFun():\n",
    "    print(value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4, 5]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapPartitions\n",
    "# Return a new RDD by applying a function to each partition of this RDD\n",
    "# mapPartitions(f, preservesPartitioning=False)\n",
    "\n",
    "x = sc.parallelize([1,2,3,4,5],2)\n",
    "x.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(iterator):\n",
    "    yield sum(iterator); yield 45\n",
    "    \n",
    "y = x.mapPartitions(func1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 45], [12, 45]]\n"
     ]
    }
   ],
   "source": [
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [12]]\n"
     ]
    }
   ],
   "source": [
    "def func2(iterator):\n",
    "    yield sum(iterator)\n",
    "    \n",
    "y = x.mapPartitions(func2)\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 3)], [(1, 12)]]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitionsWithIndex\n",
    "# Return a new RDD by applying a function to each partition of this RDD,while tracking the index of the original partition\n",
    "# mapPartitionsWithIndex(f, preservesPartitioning=False)\n",
    "def func3(partitionIndex, Iterator1):\n",
    "    yield (partitionIndex, sum(Iterator1))\n",
    "    \n",
    "\n",
    "yi = x.mapPartitionsWithIndex(func3)\n",
    "print(yi.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "# Return a new RDD containing a statistical sample of the original RDD\n",
    "# sample(withReplacement, fraction, seed=None)\n",
    "\n",
    "xs = sc.parallelize([1,2,3,4,5])\n",
    "ys = xs.sample(False,0.8,40)\n",
    "ys.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glom()\n",
    "# Return an RDD created by coalescing all elements within each partition into a list.\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "sorted(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union\n",
    "# Return a new RDD containing all items from two original RDDs. Duplicates are not culled.\n",
    "# union(otherRDD)\n",
    "\n",
    "x0 = sc.parallelize([1,2,5,4],2)\n",
    "y0 = sc.parallelize([3,5,6],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [5, 4], [3], [5, 6]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z0 = x0.union(y0)\n",
    "z0.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 4, 3, 5, 6]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join\n",
    "# Return a new RDD containing all pairs of elements having the same key in the original RDDs\n",
    "# union(otherRDD, numPartitions=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "xj = sc.parallelize([('a',1),('b',5),('c',7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "yj = sc.parallelize([('a',6),('a',9),('c',8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', (7, 8)), ('a', (1, 6)), ('a', (1, 9))]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zj = xj.join(yj)\n",
    "zj.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct\n",
    "xd = sc.parallelize([1,2,3,3,4])\n",
    "xd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COALESCE\n",
    "# Return a new RDD which is reduced to a smaller number of partitions\n",
    "# coalesce(numPartitions, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = sc.parallelize([1,2,35,4,6,7,8,7,8,9],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [35, 4], [6, 7], [8, 7, 8, 9]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [35, 4], [6, 7, 8, 7, 8, 9]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc = xc.coalesce(3)\n",
    "yc.glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'Anil'), ('A', 'Ankush'), ('D', 'Dhruvi'), ('H', 'Himu')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KEYBY\n",
    "# Create a Pair RDD, forming one pair for each item in the original RDD. The pair’s key is calculated from the value via a user-supplied function.\n",
    "# keyBy(f)\n",
    "\n",
    "xk = sc.parallelize(['Anil','Ankush','Dhruvi','Himu'])\n",
    "yk = xk.keyBy(lambda a: a[0])\n",
    "yk.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitionBy\n",
    "# Return a new RDD with the specified number of partitions, placing original items into the partition returned by a user supplied function\n",
    "# partitionBy(numPartitions, partitioner=portable_hash)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = sc.parallelize([('A', 'Anil'), ('A', 'Ankush'), ('D', 'Dhruvi'), ('H', 'Himu')],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "yx = xx.partitionBy(2, lambda a: 0 if a[0] < 'E' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('A', 'Anil')], [('A', 'Ankush')], [('D', 'Dhruvi'), ('H', 'Himu')]]\n"
     ]
    }
   ],
   "source": [
    "print(xx.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('A', 'Anil'), ('A', 'Ankush'), ('D', 'Dhruvi')], [('H', 'Himu')]]\n"
     ]
    }
   ],
   "source": [
    "print(yx.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip\n",
    "# Return a new RDD containing pairs whose key is the item in the original RDD, and whose value is that item’s corresponding element (same partition, same index) in a second RDD\n",
    "# zip(otherRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.map(lambda a: a*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.zip(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (3, 9), (5, 25)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
